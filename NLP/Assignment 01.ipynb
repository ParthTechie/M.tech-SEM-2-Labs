{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd70231b",
   "metadata": {},
   "source": [
    "### Name : D Vamsidhar\n",
    "### PRN: 24070149005\n",
    "### NLP Assignment 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb3f42",
   "metadata": {},
   "source": [
    "# Part A: Basics of NLP & Pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e4d1b",
   "metadata": {},
   "source": [
    "## **Natural Language Processing (NLP) Pipeline**\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that enables machines to understand, interpret, and generate human language. The NLP pipeline consists of several key steps:\n",
    "\n",
    "### **1. Text Preprocessing**\n",
    "Text preprocessing is crucial for cleaning and preparing raw text data before analysis. It includes:\n",
    "\n",
    "#### **a. Tokenization**\n",
    "- Splitting text into smaller units called tokens (words, subwords, or sentences).\n",
    "- Example: \"NLP is amazing!\" → [\"NLP\", \"is\", \"amazing\", \"!\"]\n",
    "\n",
    "#### **b. Stopword Removal**\n",
    "- Removing commonly used words that do not contribute to meaning (e.g., \"the\", \"is\", \"and\").\n",
    "- Example: \"This is an example sentence\" → [\"example\", \"sentence\"]\n",
    "\n",
    "#### **c. Stemming and Lemmatization**\n",
    "- **Stemming:** Reduces words to their root form by chopping suffixes (e.g., \"running\" → \"run\").\n",
    "- **Lemmatization:** Converts words to their base form using a vocabulary (e.g., \"better\" → \"good\").\n",
    "\n",
    "#### **d. Part-of-Speech (POS) Tagging**\n",
    "- Assigning word categories such as noun, verb, adjective, etc.\n",
    "- Example: \"She runs fast.\" → [(\"She\", PRON), (\"runs\", VERB), (\"fast\", ADV)]\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Feature Engineering**\n",
    "Feature extraction transforms text into numerical representations for machine learning models.\n",
    "\n",
    "#### **a. Bag of Words (BoW)**\n",
    "- Represents text as a frequency-based vector of words.\n",
    "\n",
    "#### **b. TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "- Measures the importance of words in a document relative to a collection of documents.\n",
    "\n",
    "#### **c. Word Embeddings (Word2Vec, GloVe, BERT)**\n",
    "- Captures contextual meaning and relationships between words in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Model Training**\n",
    "Machine learning or deep learning models are trained on processed text data.\n",
    "\n",
    "#### **a. Traditional ML Models**\n",
    "- Naïve Bayes, SVM, Random Forest for text classification.\n",
    "\n",
    "#### **b. Deep Learning Models**\n",
    "- RNNs, LSTMs, Transformers (BERT, GPT) for NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Model Evaluation**\n",
    "Evaluating the NLP model using metrics such as:\n",
    "\n",
    "- **Accuracy, Precision, Recall, F1-score** (for classification tasks)\n",
    "- **BLEU Score, ROUGE Score** (for text generation and summarization)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Deployment and Real-world Application**\n",
    "After training and evaluation, NLP models are deployed for various applications:\n",
    "\n",
    "- **Chatbots & Virtual Assistants** (e.g., Siri, Alexa)\n",
    "- **Sentiment Analysis** (e.g., product reviews)\n",
    "- **Machine Translation** (e.g., Google Translate)\n",
    "- **Speech-to-Text & Text-to-Speech** (e.g., Voice Assistants)\n",
    "- **Summarization & Question Answering** (e.g., News summarization)\n",
    "\n",
    "\n",
    "This structured NLP pipeline helps in various real-world applications by enabling machines to understand human language efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf44ca6",
   "metadata": {},
   "source": [
    "## **Real-World Applications of Natural Language Processing (NLP)**\n",
    "\n",
    "Natural Language Processing (NLP) has a wide range of applications across different industries. Below are three key real-world applications and the role of the NLP pipeline in each.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Chatbots and Virtual Assistants**\n",
    "Chatbots and virtual assistants, such as Siri, Alexa, and Google Assistant, use NLP to understand and respond to user queries in a conversational manner.\n",
    "\n",
    "### **Role of NLP Pipeline**\n",
    "1. **Text Preprocessing**:  \n",
    "   - Tokenization, stopword removal, and lemmatization help clean and structure user input.  \n",
    "   - Named Entity Recognition (NER) identifies key information (e.g., names, dates, locations).  \n",
    "\n",
    "2. **Feature Extraction**:  \n",
    "   - Word embeddings (e.g., BERT, Word2Vec) convert text into numerical vectors.  \n",
    "\n",
    "3. **Model Training**:  \n",
    "   - Transformer-based models (GPT, BERT) process the query and generate context-aware responses.  \n",
    "   - Intent classification models categorize queries (e.g., setting reminders, checking weather).  \n",
    "\n",
    "4. **Evaluation**:  \n",
    "   - Models are fine-tuned based on user feedback and accuracy metrics.  \n",
    "\n",
    "5. **Deployment**:  \n",
    "   - The chatbot or assistant interacts with users in real-time and continuously improves via reinforcement learning.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Sentiment Analysis in Customer Feedback**\n",
    "Sentiment analysis helps businesses analyze customer opinions and reviews to improve products and services.\n",
    "\n",
    "### **Role of NLP Pipeline**\n",
    "1. **Text Preprocessing**:  \n",
    "   - Tokenization, stopword removal, and stemming/lemmatization clean customer feedback.  \n",
    "\n",
    "2. **Feature Extraction**:  \n",
    "   - TF-IDF or word embeddings represent text numerically for analysis.  \n",
    "\n",
    "3. **Model Training**:  \n",
    "   - Machine learning models (Naïve Bayes, SVM) or deep learning models (LSTMs, Transformers) classify sentiment as positive, negative, or neutral.  \n",
    "\n",
    "4. **Evaluation**:  \n",
    "   - Accuracy, F1-score, and confusion matrices help measure performance.  \n",
    "\n",
    "5. **Deployment**:  \n",
    "   - The trained model is integrated into business dashboards for real-time analysis of customer sentiment.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Machine Translation (e.g., Google Translate)**\n",
    "Machine translation enables automatic language conversion, allowing global communication.\n",
    "\n",
    "### **Role of NLP Pipeline**\n",
    "1. **Text Preprocessing**:  \n",
    "   - Tokenization and sentence segmentation prepare input text.  \n",
    "\n",
    "2. **Feature Extraction**:  \n",
    "   - Word embeddings capture semantic meaning across different languages.  \n",
    "\n",
    "3. **Model Training**:  \n",
    "   - Sequence-to-sequence models (LSTMs, Transformers) learn language mappings.  \n",
    "   - Attention mechanisms improve context understanding in long sentences.  \n",
    "\n",
    "4. **Evaluation**:  \n",
    "   - BLEU and ROUGE scores measure translation accuracy.  \n",
    "\n",
    "5. **Deployment**:  \n",
    "   - The model is deployed in translation apps, improving over time with user feedback.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "The NLP pipeline plays a critical role in various applications by processing, understanding, and generating human language. From chatbots to sentiment analysis and machine translation, NLP continues to revolutionize the way machines interact with humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afffd2",
   "metadata": {},
   "source": [
    "# Part B: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d36548",
   "metadata": {},
   "source": [
    "# **Word-Level vs. Sentence-Level Tokenization**\n",
    "\n",
    "Tokenization is a fundamental step in the Natural Language Processing (NLP) pipeline that involves splitting text into meaningful units. It can be performed at different levels, such as word-level and sentence-level.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Word-Level Tokenization**\n",
    "Word-level tokenization breaks a sentence into individual words or tokens. It is useful for tasks like text analysis, sentiment analysis, and word frequency calculations.\n",
    "\n",
    "### **Example:**\n",
    "#### **Input Text:**  \n",
    "*\"Natural Language Processing is fascinating! It enables machines to understand human language.\"*\n",
    "\n",
    "#### **Word-Level Tokens:**  \n",
    "`[\"Natural\", \"Language\", \"Processing\", \"is\", \"fascinating\", \"!\", \"It\", \"enables\", \"machines\", \"to\", \"understand\", \"human\", \"language\", \".\"]`\n",
    "\n",
    "### **Key Features:**\n",
    "- Splits text into individual words, including punctuation marks as separate tokens.\n",
    "- Helps in tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging.\n",
    "- Challenges include handling contractions (e.g., \"don't\" → [\"do\", \"n't\"]) and special characters.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Sentence-Level Tokenization**\n",
    "Sentence-level tokenization divides a text into individual sentences. It is useful for applications like text summarization, machine translation, and sentiment analysis at a document level.\n",
    "\n",
    "### **Example:**\n",
    "#### **Input Text:**  \n",
    "*\"Natural Language Processing is fascinating! It enables machines to understand human language.\"*\n",
    "\n",
    "#### **Sentence-Level Tokens:**  \n",
    "`[\"Natural Language Processing is fascinating!\", \"It enables machines to understand human language.\"]`\n",
    "\n",
    "### **Key Features:**\n",
    "- Splits text based on sentence boundaries.\n",
    "- Handles punctuation such as periods, exclamation marks, and question marks.\n",
    "- Challenges include detecting abbreviations (e.g., \"Dr.\", \"etc.\") where periods do not indicate sentence boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "| **Feature**        | **Word-Level Tokenization** | **Sentence-Level Tokenization** |\n",
    "|--------------------|---------------------------|--------------------------------|\n",
    "| **Definition**     | Splits text into words.    | Splits text into sentences.   |\n",
    "| **Example Output** | `[\"Natural\", \"Language\", \"Processing\", \"is\", \"fascinating\", \"!\"]` | `[\"Natural Language Processing is fascinating!\"]` |\n",
    "| **Use Cases**      | POS tagging, Named Entity Recognition, Machine Translation. | Text summarization, Sentiment Analysis, Document Segmentation. |\n",
    "| **Challenges**     | Handling punctuation, contractions. | Handling abbreviations, sentence boundaries. |\n",
    "\n",
    "Both tokenization techniques play a crucial role in NLP, depending on the requirements of the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df290f",
   "metadata": {},
   "source": [
    "## Write Python code using a library (e.g., NLTK or SpaCy) to perform tokenization on the text mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3079ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Word Tokenization: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!', 'It', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n",
      "NLTK Sentence Tokenization: ['Natural Language Processing is fascinating!', 'It enables machines to understand human language.']\n",
      "\n",
      "spaCy Word Tokenization: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!', 'It', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n",
      "spaCy Sentence Tokenization: ['Natural Language Processing is fascinating!', 'It enables machines to understand human language.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is fascinating! It enables machines to understand human language.\"\n",
    "\n",
    "# Using NLTK for tokenization\n",
    "nltk.download('punkt')  # Ensure necessary data is downloaded\n",
    "word_tokens_nltk = word_tokenize(text)\n",
    "sentence_tokens_nltk = sent_tokenize(text)\n",
    "\n",
    "print(\"NLTK Word Tokenization:\", word_tokens_nltk)\n",
    "print(\"NLTK Sentence Tokenization:\", sentence_tokens_nltk)\n",
    "\n",
    "# Using spaCy for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "word_tokens_spacy = [token.text for token in doc]\n",
    "sentence_tokens_spacy = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(\"\\nspaCy Word Tokenization:\", word_tokens_spacy)\n",
    "print(\"spaCy Sentence Tokenization:\", sentence_tokens_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f191420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.2 MB 1.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/12.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.3/12.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/12.2 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/12.2 MB 1.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.4/12.2 MB 1.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.6/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.7/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.9/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.5/12.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.5/12.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.0/12.2 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.3/12.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.9/12.2 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.1/12.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.2/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.7/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.0/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.2/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.2 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/6.2 MB 2.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.1/6.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 2.9/6.2 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.4/6.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.2 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 3.5 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6b3386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.8/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 5.0/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 5.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 6.3/12.8 MB 1.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 6.6/12.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05ac4f",
   "metadata": {},
   "source": [
    "# Part C: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2fb00",
   "metadata": {},
   "source": [
    "## **Comparison: Stemming vs. Lemmatization**\n",
    "\n",
    "Both **stemming** and **lemmatization** are text preprocessing techniques used in Natural Language Processing (NLP) to reduce words to their base or root form. However, they differ in their approach and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Stemming**\n",
    "Stemming is a rule-based process of removing suffixes from words to obtain their root form. It often results in non-linguistic root words.\n",
    "\n",
    "### **Example:**\n",
    "- \"Running\" → \"Run\"\n",
    "- \"Happily\" → \"Happili\"\n",
    "- \"Studies\" → \"Studi\"\n",
    "\n",
    "### **Key Characteristics:**\n",
    "- Uses heuristic rules to chop off prefixes or suffixes.\n",
    "- Does not consider the actual meaning of words.\n",
    "- Produces stem words that may not always be valid words.\n",
    "\n",
    "### **Common Stemmers:**\n",
    "- Porter Stemmer\n",
    "- Snowball Stemmer\n",
    "- Lancaster Stemmer\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Lemmatization**\n",
    "Lemmatization reduces a word to its **base or dictionary form (lemma)** using linguistic rules and vocabulary.\n",
    "\n",
    "### **Example:**\n",
    "- \"Running\" → \"Run\"\n",
    "- \"Happily\" → \"Happy\"\n",
    "- \"Studies\" → \"Study\"\n",
    "- \"Better\" → \"Good\"\n",
    "\n",
    "### **Key Characteristics:**\n",
    "- Considers the **context** and meaning of the word.\n",
    "- Uses **lexical databases** like WordNet.\n",
    "- Produces valid words that exist in the dictionary.\n",
    "\n",
    "### **Common Lemmatizers:**\n",
    "- WordNet Lemmatizer (NLTK)\n",
    "- SpaCy Lemmatizer\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Key Differences:**\n",
    "\n",
    "| **Feature**       | **Stemming**                          | **Lemmatization**                   |\n",
    "|-------------------|--------------------------------------|--------------------------------------|\n",
    "| **Definition**    | Removes prefixes/suffixes to get a root form. | Converts words to their dictionary form (lemma). |\n",
    "| **Approach**      | Rule-based heuristic approach.       | Dictionary and linguistic-based approach. |\n",
    "| **Output Words**  | May not be valid words (e.g., \"happi\" instead of \"happy\"). | Always produces valid words (e.g., \"happy\" instead of \"happi\"). |\n",
    "| **Context Aware** | No, simply trims words.             | Yes, considers part of speech and meaning. |\n",
    "| **Accuracy**      | Less accurate, faster processing.   | More accurate, slightly slower. |\n",
    "| **Example** (for \"running\") | \"Runn\" | \"Run\" |\n",
    "| **Example** (for \"better\") | \"Better\" | \"Good\" |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. When to Use Which?**\n",
    "- **Use Stemming** when you need **faster processing** and approximate results (e.g., search engines).\n",
    "- **Use Lemmatization** when **accuracy is important** (e.g., NLP tasks like Named Entity Recognition and Machine Translation).\n",
    "\n",
    "### **Conclusion**\n",
    "While **stemming** is faster but less precise, **lemmatization** is more accurate but computationally expensive. The choice depends on the application and accuracy requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858233bf",
   "metadata": {},
   "source": [
    "### Given the following words, perform stemming and lemmatization. Use Python for implementation and include the code snippet and output:\n",
    "* Playing\n",
    "* Studies\n",
    "* Happier\n",
    "* Knives\n",
    "* Children\n",
    "* Easily\n",
    "* Faster\n",
    "* Caring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d352e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['Playing', 'Studies', 'Happier', 'Knives', 'Children', 'Easily', 'Faster', 'Caring']\n",
      "\n",
      "Stemmed Words: ['play', 'studi', 'happier', 'knive', 'children', 'easili', 'faster', 'care']\n",
      "\n",
      "Lemmatized Words (Verb Lemmatization): ['play', 'study', 'happier', 'knives', 'children', 'easily', 'faster', 'care']\n",
      "\n",
      "Lemmatized Words (Noun Lemmatization): ['playing', 'study', 'happier', 'knife', 'child', 'easily', 'faster', 'caring']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# List of words\n",
    "words = [\"Playing\", \"Studies\", \"Happier\", \"Knives\", \"Children\", \"Easily\", \"Faster\", \"Caring\"]\n",
    "\n",
    "# Stemming using PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word.lower()) for word in words]\n",
    "\n",
    "# Lemmatization using WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower(), pos='v') for word in words]  # Using 'v' for verb lemma\n",
    "lemmatized_words_noun = [lemmatizer.lemmatize(word.lower(), pos='n') for word in words]  # Using 'n' for noun lemma\n",
    "\n",
    "# Print results\n",
    "print(\"Original Words:\", words)\n",
    "print(\"\\nStemmed Words:\", stemmed_words)\n",
    "print(\"\\nLemmatized Words (Verb Lemmatization):\", lemmatized_words)\n",
    "print(\"\\nLemmatized Words (Noun Lemmatization):\", lemmatized_words_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4995fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
