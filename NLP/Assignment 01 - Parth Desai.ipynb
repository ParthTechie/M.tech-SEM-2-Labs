{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd70231b",
   "metadata": {},
   "source": [
    "### Name : Parth Desai\n",
    "### PRN: 24070149017\n",
    "### NLP ASSIGNMENT I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb3f42",
   "metadata": {},
   "source": [
    "# Part A: Basics of NLP & Pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e4d1b",
   "metadata": {},
   "source": [
    "<h1>Key Components of the Natural Language Processing (NLP) Pipeline</h1>\n",
    "\n",
    "<p>Natural Language Processing (NLP) is a branch of artificial intelligence (AI) that deals with the interaction between computers and human languages. The NLP pipeline refers to the series of steps or stages through which raw text data is processed to extract meaningful information. Below are the main components of the NLP pipeline:</p>\n",
    "\n",
    "<h2>1. Text Preprocessing</h2>\n",
    "<p>Text preprocessing is the first and most important step in the NLP pipeline. It involves cleaning the raw text data to make it suitable for further processing. Common techniques include:</p>\n",
    "<ul>\n",
    "    <li><strong>Tokenization</strong>: Breaking down a text into smaller units like words, sentences, or subwords.</li>\n",
    "    <li><strong>Lowercasing</strong>: Converting all characters to lowercase to avoid case-sensitive mismatches.</li>\n",
    "    <li><strong>Removing Stop Words</strong>: Filtering out common words (like \"the\", \"is\", etc.) that do not add meaningful context.</li>\n",
    "    <li><strong>Removing Punctuation</strong>: Eliminating punctuation marks that are irrelevant to understanding the meaning of the text.</li>\n",
    "    <li><strong>Stemming or Lemmatization</strong>: Reducing words to their root forms (e.g., \"running\" becomes \"run\"). Lemmatization uses vocabulary and morphology to ensure the root form is meaningful.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>2. Feature Extraction</h2>\n",
    "<p>Feature extraction involves transforming the raw text data into numerical features that machine learning algorithms can understand. Two common methods are:</p>\n",
    "<ul>\n",
    "    <li><strong>Bag of Words (BoW)</strong>: This model represents text as a collection of words without considering the order or structure of the words. Each word is treated as a feature, and its frequency in the text is counted.</li>\n",
    "    <li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>: This method adjusts the frequency of words based on their importance across multiple documents. It helps reduce the impact of commonly occurring words.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>3. Part-of-Speech Tagging</h2>\n",
    "<p>Part-of-Speech (POS) tagging involves identifying the grammatical categories of each word in a sentence (e.g., noun, verb, adjective). This step helps understand the syntactic structure of a sentence, which is crucial for further analysis.</p>\n",
    "\n",
    "<h2>4. Named Entity Recognition (NER)</h2>\n",
    "<p>NER is the process of identifying named entities in the text, such as the names of people, organizations, locations, dates, etc. This step helps to extract specific information from unstructured text.</p>\n",
    "\n",
    "<h2>5. Dependency Parsing</h2>\n",
    "<p>Dependency parsing involves analyzing the grammatical structure of a sentence to establish relationships between words. It helps in understanding the syntactic dependencies between words, which is crucial for tasks such as question answering or text summarization.</p>\n",
    "\n",
    "<h2>6. Sentiment Analysis</h2>\n",
    "<p>Sentiment analysis determines the sentiment or emotional tone expressed in a text (e.g., positive, negative, neutral). This is widely used in applications like social media monitoring and customer feedback analysis.</p>\n",
    "\n",
    "<h2>7. Machine Learning or Deep Learning Models</h2>\n",
    "<p>Once features are extracted, machine learning or deep learning models are used to analyze the data and make predictions or classify text. Common models include:</p>\n",
    "<ul>\n",
    "    <li><strong>Naive Bayes</strong>: A probabilistic model based on Bayes' theorem, often used for text classification.</li>\n",
    "    <li><strong>Support Vector Machines (SVM)</strong>: A supervised learning model used for classification tasks.</li>\n",
    "    <li><strong>Deep Neural Networks</strong>: Advanced models such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformer-based models (e.g., BERT) are also commonly used in NLP tasks.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Practical Example: Sentiment Analysis of Movie Reviews</h2>\n",
    "<p>Consider a scenario where we want to perform sentiment analysis on a collection of movie reviews. The steps in the NLP pipeline would look like this:</p>\n",
    "<ul>\n",
    "    <li><strong>Text Preprocessing</strong>: Clean the text by tokenizing, lowercasing, removing stop words and punctuation, and performing stemming or lemmatization.</li>\n",
    "    <li><strong>Feature Extraction</strong>: Convert the text data into numerical features using the Bag of Words or TF-IDF method.</li>\n",
    "    <li><strong>Sentiment Analysis</strong>: Apply a machine learning model (e.g., Naive Bayes or an LSTM network) to classify the sentiment of each review (positive, negative, or neutral).</li>\n",
    "</ul>\n",
    "\n",
    "<p>In this example, the final output would be a prediction of the sentiment for each review, such as \"positive\" or \"negative\".</p>\n",
    "\n",
    "<h2>Conclusion</h2>\n",
    "<p>The NLP pipeline consists of several crucial stages, including text preprocessing, feature extraction, part-of-speech tagging, named entity recognition, and more. Understanding each of these components helps build robust NLP models capable of solving a wide range of language-related tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf44ca6",
   "metadata": {},
   "source": [
    "<h1>Real-World Applications of Natural Language Processing (NLP)</h1>\n",
    "\n",
    "<p>Natural Language Processing (NLP) is a versatile field that finds applications across various industries. Below are three real-world applications of NLP, along with a description of how the NLP pipeline plays a crucial role in each application.</p>\n",
    "\n",
    "<h2>1. Sentiment Analysis in Social Media Monitoring</h2>\n",
    "<p><strong>Application:</strong> Sentiment analysis is used to analyze opinions, feedback, and emotions expressed on social media platforms such as Twitter, Facebook, and Instagram. Companies often use sentiment analysis to monitor brand reputation, understand customer satisfaction, and track public reactions to events or campaigns.</p>\n",
    "\n",
    "<p><strong>Role of NLP Pipeline:</strong> The NLP pipeline plays a key role in sentiment analysis by processing and understanding large amounts of unstructured social media text. Here's how the pipeline contributes:</p>\n",
    "<ul>\n",
    "    <li><strong>Text Preprocessing</strong>: Text from social media posts needs to be cleaned by removing irrelevant content, such as hashtags, mentions, and special characters. Tokenization, lowercasing, and removing stop words ensure that only meaningful content is considered.</li>\n",
    "    <li><strong>Feature Extraction</strong>: Features are extracted using methods like TF-IDF or word embeddings (e.g., Word2Vec or GloVe) to convert text into a numerical format that a machine learning model can understand.</li>\n",
    "    <li><strong>Sentiment Classification</strong>: A trained machine learning or deep learning model classifies the sentiment as positive, negative, or neutral based on the extracted features, helping businesses understand public opinion.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>2. Chatbots and Virtual Assistants</h2>\n",
    "<p><strong>Application:</strong> Virtual assistants like Siri, Alexa, and Google Assistant use NLP to understand and respond to user queries in natural language. These assistants are designed to assist users with tasks such as setting reminders, answering questions, or controlling smart devices.</p>\n",
    "\n",
    "<p><strong>Role of NLP Pipeline:</strong> The NLP pipeline is central to processing and responding to user input in chatbots and virtual assistants. Here's how it works in this application:</p>\n",
    "<ul>\n",
    "    <li><strong>Text Preprocessing</strong>: The user’s speech or text input is first preprocessed to eliminate noise, such as punctuation, informal language, or slang, to ensure clarity in understanding the message.</li>\n",
    "    <li><strong>Named Entity Recognition (NER)</strong>: NER identifies key entities such as dates, times, locations, and names within the query. For example, if a user asks, \"What is the weather like in New York tomorrow?\", NER would identify \"New York\" as a location and \"tomorrow\" as a time reference.</li>\n",
    "    <li><strong>Intent Recognition</strong>: The intent behind the user’s query is determined, whether it's a request for information, a command, or a question. This step often relies on models trained using supervised learning techniques.</li>\n",
    "    <li><strong>Response Generation</strong>: Based on the recognized intent and extracted entities, a suitable response is generated, which could be retrieved from a database or generated dynamically using models like GPT (Generative Pretrained Transformers).</li>\n",
    "</ul>\n",
    "\n",
    "<h2>3. Machine Translation</h2>\n",
    "<p><strong>Application:</strong> Machine translation involves automatically translating text from one language to another. Popular translation services such as Google Translate and DeepL use NLP techniques to enable cross-lingual communication.</p>\n",
    "\n",
    "<p><strong>Role of NLP Pipeline:</strong> The NLP pipeline in machine translation facilitates the conversion of text between different languages. Here’s how each component of the pipeline contributes:</p>\n",
    "<ul>\n",
    "    <li><strong>Text Preprocessing</strong>: Text is cleaned and normalized to remove inconsistencies like spelling errors, extra spaces, or unnecessary punctuation, which could affect the translation accuracy.</li>\n",
    "    <li><strong>Part-of-Speech Tagging and Dependency Parsing</strong>: These techniques help analyze the grammatical structure of the source language to identify relationships between words. This is particularly important for languages with complex grammatical rules.</li>\n",
    "    <li><strong>Translation Model</strong>: A translation model, often based on deep learning techniques (e.g., Neural Machine Translation models like Seq2Seq or Transformer), is used to learn the mapping between words, phrases, and sentence structures in different languages.</li>\n",
    "    <li><strong>Post-Processing</strong>: After translation, the text is post-processed to ensure proper sentence structure, word order, and fluency in the target language. This step might involve techniques like grammar checking and reordering sentences.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion</h2>\n",
    "<p>Natural Language Processing is transforming industries by providing intelligent solutions to tasks that require understanding, interpreting, and generating human language. Whether it's sentiment analysis for brand monitoring, enabling human-like interactions with virtual assistants, or breaking language barriers with machine translation, the NLP pipeline is crucial in ensuring the success of these applications. As NLP continues to evolve, its impact on real-world applications will only grow stronger.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afffd2",
   "metadata": {},
   "source": [
    "# Part B: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d36548",
   "metadata": {},
   "source": [
    "<h1>Word-Level vs Sentence-Level Tokenization</h1>\n",
    "\n",
    "<p>Tokenization is the process of splitting text into smaller, meaningful units, known as \"tokens.\" It is a fundamental step in Natural Language Processing (NLP). There are different ways to perform tokenization, two of the most common being word-level tokenization and sentence-level tokenization. Let's explore both with examples.</p>\n",
    "\n",
    "<h2>Word-Level Tokenization</h2>\n",
    "<p><strong>Definition:</strong> Word-level tokenization involves splitting text into individual words. It breaks the text into a sequence of words or word-like units (including punctuation as separate tokens). This is useful when we need to analyze each word in a sentence separately for tasks like text classification or sentiment analysis.</p>\n",
    "\n",
    "<p><strong>Example:</strong> Consider the following text:</p>\n",
    "<blockquote>\n",
    "    \"Natural Language Processing is fascinating! It enables machines to understand human language.\"\n",
    "</blockquote>\n",
    "\n",
    "<p>Applying word-level tokenization would break this text into the following tokens:</p>\n",
    "<ul>\n",
    "    <li>Natural</li>\n",
    "    <li>Language</li>\n",
    "    <li>Processing</li>\n",
    "    <li>is</li>\n",
    "    <li>fascinating</li>\n",
    "    <li>!</li>\n",
    "    <li>It</li>\n",
    "    <li>enables</li>\n",
    "    <li>machines</li>\n",
    "    <li>to</li>\n",
    "    <li>understand</li>\n",
    "    <li>human</li>\n",
    "    <li>language</li>\n",
    "    <li>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>As you can see, each word is treated as an individual token, and punctuation marks (like \"!\" and \".\") are also considered as separate tokens. This is typical in word-level tokenization, where we are primarily concerned with the individual components of the text.</p>\n",
    "\n",
    "<h2>Sentence-Level Tokenization</h2>\n",
    "<p><strong>Definition:</strong> Sentence-level tokenization, on the other hand, involves splitting text into complete sentences. This is often used in applications where understanding the structure and meaning of entire sentences is necessary, such as in machine translation, summarization, or dialogue systems.</p>\n",
    "\n",
    "<p><strong>Example:</strong> Using the same text:</p>\n",
    "<blockquote>\n",
    "    \"Natural Language Processing is fascinating! It enables machines to understand human language.\"\n",
    "</blockquote>\n",
    "\n",
    "<p>After sentence-level tokenization, the text would be split into the following two tokens:</p>\n",
    "<ul>\n",
    "    <li>\"Natural Language Processing is fascinating!\"</li>\n",
    "    <li>\"It enables machines to understand human language.\"</li>\n",
    "</ul>\n",
    "\n",
    "<p>Here, the entire sentences are considered as individual tokens. Sentence-level tokenization helps in tasks where the meaning or context of the entire sentence is important.</p>\n",
    "\n",
    "<h2>Key Differences</h2>\n",
    "<ul>\n",
    "    <li><strong>Granularity:</strong> Word-level tokenization breaks the text into smaller units (words and punctuation), whereas sentence-level tokenization divides the text into larger units (entire sentences).</li>\n",
    "    <li><strong>Use Cases:</strong> Word-level tokenization is used in tasks like text classification, sentiment analysis, and word embeddings, where individual words carry significant meaning. Sentence-level tokenization is useful in tasks like machine translation, summarization, and speech-to-text systems where understanding full sentence structure is essential.</li>\n",
    "    <li><strong>Context:</strong> Sentence-level tokenization retains more contextual information, as it preserves the sentence's overall meaning. Word-level tokenization often removes this context, focusing on individual words.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion</h2>\n",
    "<p>Both word-level and sentence-level tokenization play vital roles in the NLP pipeline. While word-level tokenization is more granular and is used for tasks that focus on individual words, sentence-level tokenization allows for understanding the broader context by working with full sentences. Depending on the specific NLP task at hand, one approach may be more suitable than the other.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df290f",
   "metadata": {},
   "source": [
    "## Write Python code using a library (e.g., NLTK or SpaCy) to perform tokenization on the text mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3079ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Word Tokenization: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!', 'It', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n",
      "NLTK Sentence Tokenization: ['Natural Language Processing is fascinating!', 'It enables machines to understand human language.']\n",
      "\n",
      "spaCy Word Tokenization: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!', 'It', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n",
      "spaCy Sentence Tokenization: ['Natural Language Processing is fascinating!', 'It enables machines to understand human language.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is fascinating! It enables machines to understand human language.\"\n",
    "\n",
    "# Using NLTK for tokenization\n",
    "nltk.download('punkt')  # Ensure necessary data is downloaded\n",
    "word_tokens_nltk = word_tokenize(text)\n",
    "sentence_tokens_nltk = sent_tokenize(text)\n",
    "\n",
    "print(\"NLTK Word Tokenization:\", word_tokens_nltk)\n",
    "print(\"NLTK Sentence Tokenization:\", sentence_tokens_nltk)\n",
    "\n",
    "# Using spaCy for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "word_tokens_spacy = [token.text for token in doc]\n",
    "sentence_tokens_spacy = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(\"\\nspaCy Word Tokenization:\", word_tokens_spacy)\n",
    "print(\"spaCy Sentence Tokenization:\", sentence_tokens_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f191420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.2 MB 1.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/12.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.3/12.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/12.2 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/12.2 MB 1.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.4/12.2 MB 1.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.6/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.7/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.9/12.2 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.5/12.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.5/12.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.0/12.2 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.3/12.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.9/12.2 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.1/12.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.2/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.7/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.0/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.2/12.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.2 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/6.2 MB 2.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.1/6.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 2.9/6.2 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.4/6.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.2 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 3.5 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6b3386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.8/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 5.0/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 5.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 6.3/12.8 MB 1.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 6.6/12.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05ac4f",
   "metadata": {},
   "source": [
    "# Part C: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2fb00",
   "metadata": {},
   "source": [
    "## **Comparison: Stemming vs. Lemmatization**\n",
    "\n",
    "Both **stemming** and **lemmatization** are text preprocessing techniques used in Natural Language Processing (NLP) to reduce words to their base or root form. However, they differ in their approach and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Stemming**\n",
    "Stemming is a rule-based process of removing suffixes from words to obtain their root form. It often results in non-linguistic root words.\n",
    "\n",
    "### **Example:**\n",
    "- \"Running\" → \"Run\"\n",
    "- \"Happily\" → \"Happili\"\n",
    "- \"Studies\" → \"Studi\"\n",
    "\n",
    "### **Key Characteristics:**\n",
    "- Uses heuristic rules to chop off prefixes or suffixes.\n",
    "- Does not consider the actual meaning of words.\n",
    "- Produces stem words that may not always be valid words.\n",
    "\n",
    "### **Common Stemmers:**\n",
    "- Porter Stemmer\n",
    "- Snowball Stemmer\n",
    "- Lancaster Stemmer\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Lemmatization**\n",
    "Lemmatization reduces a word to its **base or dictionary form (lemma)** using linguistic rules and vocabulary.\n",
    "\n",
    "### **Example:**\n",
    "- \"Running\" → \"Run\"\n",
    "- \"Happily\" → \"Happy\"\n",
    "- \"Studies\" → \"Study\"\n",
    "- \"Better\" → \"Good\"\n",
    "\n",
    "### **Key Characteristics:**\n",
    "- Considers the **context** and meaning of the word.\n",
    "- Uses **lexical databases** like WordNet.\n",
    "- Produces valid words that exist in the dictionary.\n",
    "\n",
    "### **Common Lemmatizers:**\n",
    "- WordNet Lemmatizer (NLTK)\n",
    "- SpaCy Lemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858233bf",
   "metadata": {},
   "source": [
    "### Given the following words, perform stemming and lemmatization. Use Python for implementation and include the code snippet and output:\n",
    "* Playing\n",
    "* Studies\n",
    "* Happier\n",
    "* Knives\n",
    "* Children\n",
    "* Easily\n",
    "* Faster\n",
    "* Caring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d352e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['Playing', 'Studies', 'Happier', 'Knives', 'Children', 'Easily', 'Faster', 'Caring']\n",
      "\n",
      "Stemmed Words: ['play', 'studi', 'happier', 'knive', 'children', 'easili', 'faster', 'care']\n",
      "\n",
      "Lemmatized Words (Verb Lemmatization): ['play', 'study', 'happier', 'knives', 'children', 'easily', 'faster', 'care']\n",
      "\n",
      "Lemmatized Words (Noun Lemmatization): ['playing', 'study', 'happier', 'knife', 'child', 'easily', 'faster', 'caring']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# List of words\n",
    "words = [\"Playing\", \"Studies\", \"Happier\", \"Knives\", \"Children\", \"Easily\", \"Faster\", \"Caring\"]\n",
    "\n",
    "# Stemming using PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word.lower()) for word in words]\n",
    "\n",
    "# Lemmatization using WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower(), pos='v') for word in words]  # Using 'v' for verb lemma\n",
    "lemmatized_words_noun = [lemmatizer.lemmatize(word.lower(), pos='n') for word in words]  # Using 'n' for noun lemma\n",
    "\n",
    "# Print results\n",
    "print(\"Original Words:\", words)\n",
    "print(\"\\nStemmed Words:\", stemmed_words)\n",
    "print(\"\\nLemmatized Words (Verb Lemmatization):\", lemmatized_words)\n",
    "print(\"\\nLemmatized Words (Noun Lemmatization):\", lemmatized_words_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4995fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
